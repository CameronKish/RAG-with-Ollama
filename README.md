# RAG-with-Ollama
Retrieval Augmented Generation (RAG) is what gives small LLMs with small context windows the capability to do infinitely more. This code acts as my learning process for understanding RAG and implementing it with Ollama, so I can query my files from anywhere without need for the internet. The use case for this technology is allowing businesses to create safe and local systems to query their own files without fear of big tech stealing their data.

Future enhancement ideas: Implement docker so the LLM can be run on a remote server and not suck up all the processing power of the user's computer. 
