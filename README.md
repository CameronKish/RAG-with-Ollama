# RAG-with-Ollama
Retrieval Augmented Generation (RAG) is what gives small LLMs with small context windows the capability to do infinitely more. This code acts as my learning process for understanding RAG and implementing it with Ollama, so I can query my files from anywhere without need for the internet. The use case for this technology is allowing businesses to create safe and local systems to query their own files without fear of big tech stealing their data.

**Future project enhancement:** Implement docker so the LLM can be run on a remote server and not suck up all the processing power of the user's computer.


#### Looking at the Paul Grahm text as an example, here is an example query and output:
Query: "In what context are aliens talked about in this document?"

Local LLM Response: "Aliens are discussed in the context of their potential knowledge and understanding of concepts, such as mathematical theorems and programming languages like Lisp. It is suggested that they may have discovered these concepts independently or learned them through observation. The text does not provide any further information about aliens or their existence."
